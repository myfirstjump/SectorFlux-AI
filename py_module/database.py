import os
import time
import urllib.parse
import pandas as pd
import uuid
from sqlalchemy import create_engine, text
from loguru import logger
from py_module.config import Configuration 

class DatabaseManipulation:
    def __init__(self, config):
        self.config = config
        self.engine = self._create_engine()

    def _create_engine(self):
        """
        å»ºç«‹ SQL Server é€£ç·šå¼•æ“ (é–‹å•Ÿ fast_executemany ä»¥åŠ é€Ÿå¯«å…¥)
        """
        params = urllib.parse.quote_plus(
            f"DRIVER={{ODBC Driver 18 for SQL Server}};"
            f"SERVER={self.config.DB_HOST},{self.config.DB_PORT};"
            f"DATABASE={self.config.DB_NAME};"
            f"UID={self.config.DB_USER};"
            f"PWD={self.config.DB_PASS};"
            f"TrustServerCertificate=yes;"
        )
        return create_engine(f"mssql+pyodbc:///?odbc_connect={params}", fast_executemany=True)

    def upsert_market_data(self, df, table_name="Fact_DailyPrice"):
        """
        å°‡çˆ¬èŸ²è³‡æ–™é€é TempDB + MERGE å¯«å…¥ (å…·å‚™æ­»çµé‡è©¦èˆ‡æš´åŠ›è¦†è“‹æ©Ÿåˆ¶)
        """
        if df is None or df.empty:
            return

        # ç¢ºä¿ DataFrame æ¬„ä½èˆ‡è³‡æ–™åº«å°é½Š
        required_cols = ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market_Cap', 'Shares_Outstanding']
        for col in required_cols:
            if col not in df.columns:
                df[col] = None 

        unique_suffix = uuid.uuid4().hex[:8]
        staging_table = f"#Staging_{table_name}_{unique_suffix}"
        
        max_retries = 3
        for attempt in range(max_retries):
            trans = None
            try:
                with self.engine.connect() as conn:
                    trans = conn.begin()
                    
                    df[required_cols].to_sql(staging_table, con=conn, if_exists='replace', index=False, 
                                             chunksize=self.config.DB_CHUNK_SIZE)
                    
                    merge_sql = text(f"""
                    MERGE INTO {table_name} AS target
                    USING {staging_table} AS source
                    ON target.Date = source.Date AND target.Symbol = source.Symbol
                    WHEN MATCHED THEN
                        UPDATE SET 
                            target.[Open] = source.[Open],
                            target.High = source.High,
                            target.Low = source.Low,
                            target.[Close] = source.[Close],
                            target.Volume = source.Volume,
                            target.Market_Cap = source.Market_Cap,
                            target.Shares_Outstanding = source.Shares_Outstanding
                    WHEN NOT MATCHED THEN
                        INSERT (Date, Symbol, [Open], High, Low, [Close], Volume, Market_Cap, Shares_Outstanding)
                        VALUES (source.Date, source.Symbol, source.[Open], source.High, source.Low, source.[Close], source.Volume, source.Market_Cap, source.Shares_Outstanding);
                    """)
                    
                    conn.execute(merge_sql)
                    conn.execute(text(f"DROP TABLE {staging_table}"))
                    trans.commit()
                    break 
                    
            except Exception as e:
                if trans: trans.rollback()
                error_msg = str(e)
                if '1205' in error_msg or 'Deadlock' in error_msg:
                    if attempt < max_retries - 1:
                        time.sleep((attempt + 1) * 2)
                        continue
                logger.error(f"âŒ Upsert å¤±æ•—: {error_msg}")
                raise e
    
    def upsert_etf_holdings(self, df):
        """å¯«å…¥ ETF æŒå€‰æ•¸æ“š (ä½¿ç”¨ MERGE)"""
        if df is None or df.empty: return
        
        unique_suffix = uuid.uuid4().hex[:8]
        staging_table = f"#Staging_Holdings_{unique_suffix}"
        
        try:
            with self.engine.connect() as conn:
                trans = conn.begin()
                df.to_sql(staging_table, con=conn, if_exists='replace', index=False)
                
                merge_sql = text(f"""
                MERGE INTO Fact_ETF_Holdings AS target
                USING {staging_table} AS source
                ON target.Date = source.Date AND target.ETF_Symbol = source.ETF_Symbol AND target.Holding_Symbol = source.Holding_Symbol
                WHEN MATCHED THEN
                    UPDATE SET target.[Weight] = source.[Weight], target.Shares = source.Shares, target.Updated_At = GETDATE()
                WHEN NOT MATCHED THEN
                    INSERT (Date, ETF_Symbol, Holding_Symbol, [Weight], Shares, Updated_At)
                    VALUES (source.Date, source.ETF_Symbol, source.Holding_Symbol, source.[Weight], source.Shares, GETDATE());
                """)
                
                conn.execute(merge_sql)
                conn.execute(text(f"DROP TABLE {staging_table}"))
                trans.commit()
        except Exception as e:
            logger.error(f"âŒ ETF Holdings Upsert å¤±æ•—: {e}")

    def prepare_tsf_features(self, benchmark='SPY', days_to_process=None):


        """
        [é«˜æ•ˆèƒ½] ä½¿ç”¨ SQL åŸç”ŸæŒ‡ä»¤è¨ˆç®— RSã€Log_Return_RS èˆ‡ 20æ—¥æ»¾å‹• Z-Score
        """
        logger.info(f"âš™ï¸ å•Ÿå‹• RS ç‰¹å¾µå·¥ç¨‹ (Benchmark: {benchmark})...")
        
        try:
            with self.engine.connect() as conn:
                trans = conn.begin()
                
                # 1. ç¢ºä¿æ–°æ¬„ä½å­˜åœ¨
                conn.execute(text("IF COL_LENGTH('Fact_DailyPrice', 'RS_Ratio') IS NULL ALTER TABLE Fact_DailyPrice ADD RS_Ratio FLOAT"))
                conn.execute(text("IF COL_LENGTH('Fact_DailyPrice', 'Log_Return_RS') IS NULL ALTER TABLE Fact_DailyPrice ADD Log_Return_RS FLOAT"))
                conn.execute(text("IF COL_LENGTH('Fact_DailyPrice', 'ZScore_20D') IS NULL ALTER TABLE Fact_DailyPrice ADD ZScore_20D FLOAT"))
                
                date_filter = ""
                if days_to_process:
                     date_filter = f"AND T1.Date >= DATEADD(day, -{days_to_process}, GETDATE())"

                # 2. è¨ˆç®— RS_Ratio (ç›¸å°å¼·åº¦)
                logger.info("ğŸš€ [1/3] è¨ˆç®— RS_Ratio...")
                sql_rs = text(f"""
                    UPDATE T1
                    SET T1.RS_Ratio = T1.[Close] / NULLIF(T2.[Close], 0)
                    FROM Fact_DailyPrice T1
                    INNER JOIN Fact_DailyPrice T2 ON T1.Date = T2.Date
                    WHERE T2.Symbol = :benchmark
                    {date_filter}
                """)
                conn.execute(sql_rs, {"benchmark": benchmark})

                # 3. è¨ˆç®— Log_Return_RS (å°æ•¸å ±é…¬ç‡)
                logger.info("ğŸš€ [2/3] è¨ˆç®— Log_Return_RS (å°æ•¸å ±é…¬ç‡)...")
                sql_log_ret = text(f"""
                    WITH CTE_Prev AS (
                        SELECT Date, Symbol, RS_Ratio,
                               LAG(RS_Ratio) OVER (PARTITION BY Symbol ORDER BY Date) AS Prev_RS
                        FROM Fact_DailyPrice
                    )
                    UPDATE T
                    SET T.Log_Return_RS = LOG(T.RS_Ratio / NULLIF(C.Prev_RS, 0))
                    FROM Fact_DailyPrice T
                    INNER JOIN CTE_Prev C ON T.Date = C.Date AND T.Symbol = C.Symbol
                    WHERE T.RS_Ratio IS NOT NULL AND C.Prev_RS IS NOT NULL
                    {date_filter.replace('T1.', 'T.')}
                """)
                conn.execute(sql_log_ret)

                # 4. è¨ˆç®— ZScore_20D (20æ—¥æ»¾å‹•æ¨™æº–åŒ–)
                logger.info("ğŸš€ [3/3] è¨ˆç®— 20æ—¥æ»¾å‹• ZScore_20D...")
                sql_zscore = text(f"""
                    WITH CTE_Stats AS (
                        SELECT Date, Symbol, Log_Return_RS,
                               AVG(Log_Return_RS) OVER (PARTITION BY Symbol ORDER BY Date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS Avg_20D,
                               STDEV(Log_Return_RS) OVER (PARTITION BY Symbol ORDER BY Date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS Std_20D
                        FROM Fact_DailyPrice
                    )
                    UPDATE T
                    SET T.ZScore_20D = (T.Log_Return_RS - C.Avg_20D) / NULLIF(C.Std_20D, 0)
                    FROM Fact_DailyPrice T
                    INNER JOIN CTE_Stats C ON T.Date = C.Date AND T.Symbol = C.Symbol
                    WHERE T.Log_Return_RS IS NOT NULL
                    {date_filter.replace('T1.', 'T.')}
                """)
                conn.execute(sql_zscore)
                
                trans.commit()
                logger.info("âœ… ç‰¹å¾µå·¥ç¨‹å…¨ç³»åˆ— (RS, Log_Return, Z-Score) æ‰¹é‡è¨ˆç®—å®Œç•¢ï¼")
                
        except Exception as e:
            logger.error(f"âŒ RS ç‰¹å¾µå·¥ç¨‹å¤±æ•—: {e}")
            raise e
        
    def generate_net_flux_matrix(self, past_date, now_date, target_assets=None, hedge_assets=None):
        """
        [æ­·å²äº‹å¯¦æ•´ç†] è¨ˆç®— ETF Observed Methodï¼Œå‹•æ…‹ç”¢å‡º Sankey æµé‡çŸ©é™£
        æ”¯æ´ L0/L1/L2 æ“´å……ï¼Œä¸¦è‡ªå‹•é©æ‡‰æ¨™çš„ä¹‹å‹•æ…‹ç”Ÿå‘½é€±æœŸ (Inception Dates)
        
        -. input:
            * past_date (str): è§€æ¸¬èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚
            * now_date (str): è§€æ¸¬çµæŸæ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚
            * target_assets (list[str], optional): ç›®æ¨™åˆ†ææ¨™çš„æ¸…å–®ã€‚è‹¥æœªæä¾› (None)ï¼Œé è¨­è¼‰å…¥ L0 ä¹‹ 11 å¤§æ¿å¡Š ETFã€‚
            * hedge_assets (list[str], optional): é¿éšªç·©è¡æ± æ¨™çš„æ¸…å–®ã€‚è‹¥æœªæä¾› (None)ï¼Œé è¨­è¼‰å…¥ ['BIL', 'SHV', 'TLT', 'GLD']ã€‚
            
        -. return:
            * flux_matrix (pd.DataFrame | None): ä¸€å€‹ N x N çš„è³‡é‡‘æµå‘è½‰ç§»çŸ©é™£ã€‚
                - Index (Row): è³‡é‡‘æµå‡ºæ–¹ (Source / Outflow)ã€‚
                - Column: è³‡é‡‘æµå…¥æ–¹ (Target / Inflow)ã€‚
                - Values: è½‰ç§»çš„å¯¦éš›è³‡é‡‘é¡åº¦ (åŸºæ–¼ Proportional Allocation åˆ†é…)ã€‚
                - åŒ…å«ä¸€å€‹å›ºå®šçš„ 'HEDGE' ç¯€é»ä»¥ç¢ºä¿æ•´é«”æµé‡å®ˆæ†ã€‚è‹¥æŸ¥ç„¡è³‡æ–™å‰‡å›å‚³ Noneã€‚
        """
        logger.info(f"ğŸŒŠ å•Ÿå‹• Net Flux è¨ˆç®—: {past_date} -> {now_date}")
        
        # 1. è™•ç†åƒæ•¸æ³¨å…¥ (æ”¯æ´æœªä¾† L1, L2 æ“´å……)
        if target_assets is None:
            # é è¨­ L0 æ¿å¡Š
            target_assets = ['XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLU', 'XLB', 'XLRE', 'XLC']
        if hedge_assets is None:
            hedge_assets = ['BIL', 'SHV', 'TLT', 'GLD']
            
        all_tickers = list(set(target_assets + hedge_assets))
        
        # 2. æŠ“å– Past èˆ‡ Now å…©æ—¥çš„è³‡æ–™
        symbols_str = "'" + "','".join(all_tickers) + "'"
        query = f"""
            SELECT Date, Symbol, [Close] AS Price, Market_Cap
            FROM Fact_DailyPrice
            WHERE Date IN ('{past_date}', '{now_date}')
            AND Symbol IN ({symbols_str})
        """
        try:
            df = pd.read_sql(query, self.engine)
        except Exception as e:
            logger.error(f"âŒ è®€å–è³‡æ–™å¤±æ•—: {e}")
            return None

        if df.empty:
            logger.warning("âš ï¸ æŸ¥ç„¡è³‡æ–™ï¼Œè«‹ç¢ºèªæ—¥æœŸæ˜¯å¦ç‚ºäº¤æ˜“æ—¥ã€‚")
            return None

        df['Date'] = df['Date'].astype(str)
        
        # 3. â³ ç”Ÿå‘½é€±æœŸéæ¿¾ (Dynamic Node Pruning)
        # æ‰¾å‡ºåœ¨ past_date èˆ‡ now_date éƒ½ã€ŒåŒæ™‚å­˜æ´»ä¸”æœ‰è³‡æ–™ã€çš„æ¨™çš„
        available_past = df[df['Date'] == past_date]['Symbol'].tolist()
        available_now = df[df['Date'] == now_date]['Symbol'].tolist()
        alive_tickers = list(set(available_past).intersection(set(available_now)))
        
        # éæ¿¾å‡ºæœ¬æ¬¡è¨ˆç®—çœŸæ­£æ´»èºçš„ç¯€é»
        active_targets = [s for s in target_assets if s in alive_tickers]
        active_hedges = [s for s in hedge_assets if s in alive_tickers]
        
        logger.info(f"ğŸ“Š æ´»èºç¯€é»æ•¸: ç›®æ¨™ç¾¤ {len(active_targets)} æª”, é¿éšªç¾¤ {len(active_hedges)} æª”")

        df_past = df[df['Date'] == past_date].set_index('Symbol')
        df_now = df[df['Date'] == now_date].set_index('Symbol')

        # 4. è¨ˆç®—å„åˆ¥æ´»èº ETF çš„æ·¨æµé‡ (F_net)
        f_net_dict = {}
        for sym in alive_tickers:
            mc_past = df_past.loc[sym, 'Market_Cap']
            price_past = df_past.loc[sym, 'Price']
            mc_now = df_now.loc[sym, 'Market_Cap']
            price_now = df_now.loc[sym, 'Price']
            
            if pd.notna(mc_past) and pd.notna(price_past) and price_past != 0:
                r_asset = (price_now / price_past) - 1
                # å…¬å¼ï¼šF_net = MC_actual,t - (MC_t-n * (1 + r_asset,t))
                f_net = mc_now - (mc_past * (1 + r_asset))
                f_net_dict[sym] = f_net
            else:
                f_net_dict[sym] = 0

        # 5. æ•´åˆç¯€é»ï¼šå‹•æ…‹ç›®æ¨™ç¾¤ + çµ±ä¸€çš„ Hedge Pool
        nodes_f_net = {s: f_net_dict.get(s, 0) for s in active_targets}
        # å³ä½¿ active_hedges ç‚ºç©ºï¼ŒHEDGE ç¯€é»ä»æœƒå­˜åœ¨ (å€¼ç‚º 0)ï¼Œåšç‚ºå¾ŒçºŒå®ˆæ†çš„æ•¸å­¸èª¿ç¯€æ± 
        nodes_f_net['HEDGE'] = sum(f_net_dict.get(h, 0) for h in active_hedges)

        # å€åˆ†è³‡é‡‘æµå‡º (Source) èˆ‡æµå…¥ (Target)
        outflows = {k: abs(v) for k, v in nodes_f_net.items() if v < 0}
        inflows = {k: v for k, v in nodes_f_net.items() if v > 0}
        
        total_out = sum(outflows.values())
        total_in = sum(inflows.values())

        # 6. âš–ï¸ æµé‡å®ˆæ†æ ¡æ­£ (Conservation of Flux)
        if total_out > total_in:
            diff = total_out - total_in
            inflows['HEDGE'] = inflows.get('HEDGE', 0) + diff
            total_in = sum(inflows.values())
        elif total_in > total_out:
            diff = total_in - total_out
            outflows['HEDGE'] = outflows.get('HEDGE', 0) + diff
            total_out = sum(outflows.values())

        # 7. å»ºç«‹å‹•æ…‹ N x N è½‰ç§»çŸ©é™£ (Proportional Allocation)
        matrix_nodes = active_targets + ['HEDGE']
        flux_matrix = pd.DataFrame(0.0, index=matrix_nodes, columns=matrix_nodes)

        # ä¾æ¯”ä¾‹å°‡æµå‡ºè³‡é‡‘åˆ†é…è‡³æµå…¥ç¯€é»
        if total_in > 0:
            for source, out_val in outflows.items():
                for target, in_val in inflows.items():
                    flux_matrix.loc[source, target] = out_val * (in_val / total_in)

        logger.success(f"âœ… å‹•æ…‹ Flux Matrix ç”Ÿæˆå®Œæˆ (çŸ©é™£ç¶­åº¦: {len(matrix_nodes)}x{len(matrix_nodes)}, ç¸½è½‰ç§»è³‡é‡‘: ${total_out:,.2f})")
        
        return flux_matrix
