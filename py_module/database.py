import pandas as pd
import logging
import time
from sqlalchemy import create_engine, text
import uuid  # ğŸŒŸ æ–°å¢é€™è¡Œï¼šç”¨ä¾†ç”¢ç”Ÿéš¨æ©Ÿå­—ä¸²

logger = logging.getLogger("SectorFlux_Database")

class DatabaseManipulation:
    def __init__(self, config):
        """
        åˆå§‹åŒ–è³‡æ–™åº«æ“ä½œæ¨¡çµ„
        """
        self.config = config
        try:
            # å»ºç«‹é€£ç·šæ± ã€‚fast_executemany=True æ˜¯ pyodbc å¤§é‡å¯«å…¥çš„æ•ˆèƒ½é—œéµ
            self.engine = create_engine(
                self.config.database_url,
                fast_executemany=True, 
                pool_size=5,
                max_overflow=10
            )
            logger.info("âœ… DatabaseManipulation åˆå§‹åŒ–æˆåŠŸï¼Œé€£ç·šæ± å·²å»ºç«‹ã€‚")
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«é€£ç·šåˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise e

    def upsert_market_data(self, df, table_name="Fact_DailyPrice"):
        """
        å°‡çˆ¬èŸ²è³‡æ–™é€é Staging Table + MERGE å¯«å…¥
        (å¯¦ä½œ TempDB éš”é›¢èˆ‡ Deadlock è‡ªå‹•é‡è©¦æ©Ÿåˆ¶)
        """
        import time # ç¢ºä¿æ¨¡çµ„å…§æœ‰ import time
        if df is None or df.empty:
            return

        # ğŸ›¡ï¸ é˜²ç¦¦ 1ï¼šåŠ ä¸Š '#' ç¬¦è™Ÿï¼Œå¼·åˆ¶åœ¨ tempdb å»ºç«‹å€åŸŸæš«å­˜è¡¨ï¼Œé¿é–‹ä¸»ç³»çµ±ç›®éŒ„é–å®š
        unique_suffix = uuid.uuid4().hex[:8]
        staging_table = f"#Staging_{table_name}_{unique_suffix}"
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with self.engine.connect() as conn:
                    trans = conn.begin()
                    
                    # å¯«å…¥ tempdbï¼Œå³ä¾¿ if_exists='replace' ä¹Ÿä¸æœƒå¹²æ“¾ä¸»åº«
                    df.to_sql(staging_table, con=conn, if_exists='replace', index=False, 
                              chunksize=self.config.DB_CHUNK_SIZE)
                    
                    merge_sql = text(f"""
                    MERGE INTO {table_name} AS target
                    USING {staging_table} AS source
                    ON target.Date = source.Date AND target.Symbol = source.Symbol
                    
                    WHEN MATCHED THEN
                        UPDATE SET 
                            target.[Open] = source.[Open],
                            target.High = source.High,
                            target.Low = source.Low,
                            target.[Close] = source.[Close],
                            target.Volume = source.Volume
                    
                    WHEN NOT MATCHED THEN
                        INSERT (Date, Symbol, [Open], High, Low, [Close], Volume)
                        VALUES (source.Date, source.Symbol, source.[Open], source.High, source.Low, source.[Close], source.Volume);
                    """)
                    
                    conn.execute(merge_sql)
                    
                    # é¤Šæˆå¥½ç¿’æ…£ï¼Œç”¨å®Œç«‹åˆ»æ¸…ç† tempdb ç©ºé–“
                    conn.execute(text(f"DROP TABLE {staging_table}"))
                    trans.commit()
                    break # åŸ·è¡ŒæˆåŠŸï¼Œè·³å‡ºé‡è©¦è¿´åœˆ
                    
            except Exception as e:
                # æ””æˆªéŒ¯èª¤ä¸¦ Rollback
                if 'trans' in locals() and trans is not None:
                    trans.rollback()
                    
                error_msg = str(e)
                # ğŸ›¡ï¸ é˜²ç¦¦ 2ï¼šåµæ¸¬ Error 1205 (Deadlock) ä¸¦è‡ªå‹•é‡è©¦
                if '1205' in error_msg or 'Deadlock' in error_msg:
                    if attempt < max_retries - 1:
                        logger.warning(f"âš ï¸ é­é‡é«˜é »å¯«å…¥æ­»çµ (1205)ï¼Œç³»çµ±é€€é¿ 1 ç§’å¾Œé€²è¡Œç¬¬ {attempt + 2} æ¬¡é‡è©¦...")
                        time.sleep(1)
                        continue # é€²å…¥ä¸‹ä¸€æ¬¡è¿´åœˆ
                
                # å¦‚æœä¸æ˜¯æ­»çµï¼Œæˆ–æ˜¯é‡è©¦æ¬¡æ•¸ç”¨ç›¡ï¼Œå‰‡æ‹‹å‡ºçœŸå¯¦éŒ¯èª¤
                logger.error(f"âŒ Upsert å¤±æ•—: {error_msg}")
                raise e

    def prepare_tsf_features(self, benchmark='VOO', days_to_process=40):
        """
        ã€è³‡æ–™é è™•ç†æ ¸å¿ƒã€‘å»è²å¡” (De-beta) é‚è¼¯
        åœ¨è³‡æ–™åº«ç«¯ç›´æ¥ä½¿ç”¨ CTE (Common Table Expression) é€²è¡Œé«˜æ•ˆé‹ç®—ï¼Œ
        è¨ˆç®—æ‰€æœ‰æ¨™çš„ç›¸å°æ–¼ benchmark çš„ RS (Relative Strength)ï¼Œä¸¦å„²å­˜è‡³ Fact_RS_Featuresã€‚
        
        :param benchmark: ä½œç‚ºåˆ†æ¯çš„åŸºæº–æ¨™çš„ (é è¨­ VOO)
        :param days_to_process: é‡æ–°è¨ˆç®—æœ€è¿‘ N å¤©çš„è³‡æ–™ (æ•ˆèƒ½å„ªåŒ–)ã€‚è‹¥è¨­ç‚º None å‰‡å…¨é‡è¨ˆç®— 30 å¹´ã€‚
        """
        logger.info(f"é–‹å§‹åœ¨è³‡æ–™åº«ç«¯è¨ˆç®—ç›¸å°æ–¼ {benchmark} çš„ RS (ç›¸å°å¼·åº¦) åºåˆ—...")
        
        # è™•ç†æ™‚é–“æ¿¾ç¶²é‚è¼¯ (é‡å° 30 å¹´æ­·å²å¤§çŒæ³¨çš„å½ˆæ€§é˜²å‘†)
        date_filter_sql = ""
        if days_to_process is not None:
            # æ¯æ—¥æ’ç¨‹ï¼šåªé‡ç®—æœ€è¿‘ 40 å¤©
            date_filter_sql = f"AND t1.Date >= DATEADD(day, -{days_to_process}, GETDATE())"
            logger.info(f"å•Ÿç”¨æ™‚é–“æ¿¾ç¶²ï¼šåƒ…è¨ˆç®—æœ€è¿‘ {days_to_process} å¤©ä¹‹è³‡æ–™ä»¥ç¯€çœé‹ç®—è³‡æºã€‚")
        else:
            # æ­·å²å¤§çŒæ³¨ï¼šç®—åˆ°é£½
            logger.info("âš ï¸ å•Ÿå‹•å…¨æ­·å²è³‡æ–™ RS é‡æ–°è¨ˆç®— (æ­¤å‹•ä½œå°‡åŸ·è¡Œå…¨è¡¨æƒæï¼Œå¯èƒ½éœ€è¦ä¸€è‡³å…©åˆ†é˜)...")

        sql_logic = text(f"""
        -- 1. ç¢ºä¿ç‰¹å¾µè¡¨å­˜åœ¨
        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='Fact_RS_Features' AND xtype='U')
        CREATE TABLE Fact_RS_Features (
            Date DATE NOT NULL,
            Symbol VARCHAR(20) NOT NULL,
            RS_Value FLOAT,
            PRIMARY KEY (Date, Symbol)
        );

        -- 2. ä½¿ç”¨ CTE çµåˆ MERGE é«˜æ•ˆè¨ˆç®—ä¸¦ Upsert
        WITH BenchmarkData AS (
            SELECT Date, [Close] AS BenchPrice
            FROM Fact_DailyPrice
            WHERE Symbol = '{benchmark}'
        )
        MERGE INTO Fact_RS_Features AS target
        USING (
            -- è¨ˆç®— RS_t = Price(target) / Price(VOO)
            SELECT t1.Date, t1.Symbol, (t1.[Close] / t2.BenchPrice) AS RS_Value
            FROM Fact_DailyPrice t1
            JOIN BenchmarkData t2 ON t1.Date = t2.Date
            WHERE t1.Symbol != '{benchmark}' AND t2.BenchPrice > 0
            {date_filter_sql}  -- <== å‹•æ…‹æ³¨å…¥æ™‚é–“æ¿¾ç¶²
        ) AS source
        ON target.Date = source.Date AND target.Symbol = source.Symbol
        
        WHEN MATCHED THEN
            UPDATE SET target.RS_Value = source.RS_Value
        
        WHEN NOT MATCHED THEN
            INSERT (Date, Symbol, RS_Value)
            VALUES (source.Date, source.Symbol, source.RS_Value);
        """)
        
        with self.engine.connect() as conn:
            trans = conn.begin()
            try:
                conn.execute(sql_logic)
                trans.commit()
                logger.info(f"âœ… RS ç‰¹å¾µå·¥ç¨‹ (De-beta) é‹ç®—å®Œç•¢ï¼Œå·²çµæ§‹åŒ–å„²å­˜è‡³ Fact_RS_Featuresã€‚")
            except Exception as e:
                trans.rollback()
                logger.error(f"âŒ RS ç‰¹å¾µé‹ç®—å¤±æ•—: {str(e)}")
                raise e

    def get_rs_series(self, target_ticker):
        """
        ä¾› tsf_modules.py (é æ¸¬æ¨¡çµ„) æå–å–®ä¸€æ¨™çš„ä¹‹ RS åºåˆ—
        """
        query = f"SELECT Date, RS_Value FROM Fact_RS_Features WHERE Symbol = '{target_ticker}' ORDER BY Date ASC"
        return pd.read_sql(query, self.engine)

    def save_predictions(self, layer, horizon, df_pred):
        """
        å„²å­˜æ¨¡å‹é æ¸¬çµæœ (M/Q/Y)ã€‚
        df_pred å¿…é ˆåŒ…å«: Date, Symbol, Prediction_Value
        """
        if df_pred is None or df_pred.empty:
            return
            
        table_name = f"Fact_Predictions_{layer}_{horizon}" # ä¾‹å¦‚ Fact_Predictions_L0_M
        
        # ç‚ºäº†ä¿æŒç¯„ä¾‹ç²¾ç°¡ï¼Œé€™è£¡æ¡ç”¨æœ€å–®ç´”çš„ pandas to_sql (å¯¦å‹™ä¸ŠåŒæ¨£å¯æ”¹å¯«ç‚º MERGE)
        try:
            with self.engine.connect() as conn:
                df_pred.to_sql(table_name, con=conn, if_exists='append', index=False, chunksize=self.config.DB_CHUNK_SIZE)
            logger.info(f"âœ… {layer} å±¤ {horizon} å°ºåº¦é æ¸¬çµæœå·²å¯«å…¥ {table_name}")
        except Exception as e:
            logger.error(f"âŒ å¯«å…¥é æ¸¬çµæœå¤±æ•—: {str(e)}")
            raise e