import logging
import requests
import pandas as pd
import time
import gc
import json
import gzip
from datetime import datetime, timedelta
from pathlib import Path

logger = logging.getLogger("SectorFlux_Crawler")

class FinancialCrawler:
    def __init__(self, config):
        """
        åˆå§‹åŒ–çˆ¬èŸ²æ¨¡çµ„
        :param config: ä¾†è‡ª config.py çš„ Configuration ç‰©ä»¶ï¼ŒåŒ…å« API Key èˆ‡å®‡å®™å®šç¾©
        """
        self.config = config
        self.api_key = config.FMP_API_KEY
        self.base_url = "https://financialmodelingprep.com"
        
        # æ¨™çš„å®‡å®™ (é€™äº›æ‡‰è©²å®šç¾©åœ¨ config.py ä¸­)
        self.l0_tickers = config.L0_SECTORS       # e.g., ['XLK', 'XLF', ...]
        self.l1_tickers = config.L1_THEMATICS     # e.g., ['SMH', 'ITA', ...]
        self.risk_tickers = config.RISK_PROXY     # e.g., ['BIL', 'SHV']
        self.benchmark = config.BASE_BENCHMARK    # e.g., ['VOO']
        
        # ç‚ºäº† L2ï¼Œå¯ä»¥åœ¨ config å®šç¾©è¦è¿½è¹¤çš„å¤§å‹è‚¡æ¸…å–®ï¼Œä¾‹å¦‚ SP500 æˆåˆ†è‚¡
        self.l2_tickers = config.L2_UNIVERSE      

    def fetch_all_data(self, market='us', history_days=30):
        """
        ä¾› main.py å‘¼å«çš„ä¸»å…¥å£
        """
        logger.info(f"é–‹å§‹åŸ·è¡Œ {market.upper()} å¸‚å ´æ•¸æ“šæŠ“å–ä»»å‹™...")
        
        if market == 'us':
            # çµ„åˆæ‰€æœ‰éœ€è¦æŠ“å–çš„ Ticker
            target_universe = self.config.get_all_tickers()
            logger.info(f"æœ¬æ¬¡ä»»å‹™å…±éœ€æŠ“å– {len(target_universe)} æª”æ¨™çš„")

            # 1. æŠ“å–è‚¡åƒ¹è³‡æ–™
            self._fetch_and_store_prices(target_universe, history_days)
            
            # 2. æŠ“å– ETF æŒå€‰ (ä¾› L2 æˆ–å¾ŒçºŒæ¬Šé‡åˆ†æä½¿ç”¨)
            # self._fetch_etf_holdings(self.l0_tickers + self.l1_tickers)
            
            # 3. æŠ“å– MMF ç¸½é«”ç¶“æ¿ŸæŒ‡æ¨™ (é¿éšªè³‡é‡‘åˆ¤æ–·)
            # self._fetch_macro_data()
            
        elif market == 'tw':
            logger.warning("å°è‚¡çˆ¬èŸ²æ¨¡çµ„å°šæœªå¯¦ä½œ (å°‡æ–¼å¾ŒçºŒç‰ˆæœ¬æ¥è»Œ)")
            pass

    def _fetch_and_store_prices(self, tickers, history_days):
        """
        æŠ“å–æ­·å²è‚¡åƒ¹ä¸¦ç›´æ¥å¯«å…¥ Databaseï¼Œåš´æ ¼æ§ç®¡è¨˜æ†¶é«” (æ”¯æ´ 30 å¹´ç„¡ç¸«åˆ†å¡ŠæŠ“å–)
        """
        from py_module.database import DatabaseManipulation
        db = DatabaseManipulation(self.config)

        success_count = 0
        
        for i, ticker in enumerate(tickers):
            try:
                # ---------------------------------------------------------
                # ğŸŒŸ æ–°å¢ï¼šå…·å‚™ã€Œé˜²è…æ•—ã€æª¢æŸ¥çš„æ–·é»çºŒå‚³æ©Ÿåˆ¶
                # ---------------------------------------------------------
                backup_dir = Path("/workspace/raw_backup")
                backup_file = backup_dir / f"{ticker}_30yr_backup.json.gz"
                
                if history_days > 5000 and backup_file.exists():
                    import os
                    # å–å¾—æª”æ¡ˆæœ€å¾Œä¿®æ”¹æ™‚é–“
                    file_mtime = backup_file.stat().st_mtime
                    file_age_days = (time.time() - file_mtime) / (24 * 3600)
                    
                    # åš´æ ¼é™åˆ¶ï¼šåªæœ‰ 7 å¤©å…§çš„å‚™ä»½æ‰è¢«è¦–ç‚ºå®‰å…¨ (ç„¡æ‹†è‚¡/é™¤æ¯é¢¨éšª)
                    if file_age_days <= 7:
                        logger.info(f"[{i+1}/{len(tickers)}] â™»ï¸ {ticker} å‚™ä»½æœ‰æ•ˆ (è·ä»Š {file_age_days:.1f} å¤©)ï¼Œå¾æœ¬åœ°è¼‰å…¥...")
                        with gzip.open(backup_file, 'rt', encoding='utf-8') as f:
                            cached_data = json.load(f)
                            
                        df = self._parse_fmp_json(cached_data, ticker)
                        if df is not None and not df.empty:
                            db.upsert_market_data(df)
                            success_count += 1
                        continue # è·³é API æŠ“å–
                    else:
                        logger.warning(f"[{i+1}/{len(tickers)}] âš ï¸ {ticker} å‚™ä»½å·²éæœŸ ({file_age_days:.1f} å¤© > 7 å¤©)ï¼Œå¯èƒ½å­˜åœ¨æœªèª¿æ•´ä¹‹æ‹†è‚¡è³‡æ–™ï¼Œå¼·åˆ¶é‡æ–°å‘ FMP æŠ“å–æœ€æ–°æ•¸æ“šï¼")
                # ---------------------------------------------------------

                logger.info(f"[{i+1}/{len(tickers)}] ğŸŒ æ­£åœ¨å¾ FMP API æŠ“å–: {ticker}")
                
                # æº–å‚™æ‰¿æ¥æ‰€æœ‰æ™‚é–“å€å¡Šè³‡æ–™çš„å®¹å™¨
                aggregated_data = []
                
                # ğŸŒŸ é—œéµå„ªåŒ–ï¼šå¯¦ä½œ FMP å®˜æ–¹è¦å®šçš„ã€Œ5 å¹´åˆ†å¡Šè¿´åœˆã€
                current_end_date = datetime.now()
                final_start_date = current_end_date - timedelta(days=history_days)
                
                # FMP è¦å®šæ¯æ¬¡å€é–“ä¸èƒ½è¶…é 5 å¹´ (ç´„ 1825 å¤©)
                chunk_days = 1825 
                
                while current_end_date > final_start_date:
                    current_start_date = current_end_date - timedelta(days=chunk_days)
                    # ç¢ºä¿ä¸æœƒè¶…æŠ“è¶…éä½¿ç”¨è€…è¨­å®šçš„æ­·å²å¤©æ•¸
                    if current_start_date < final_start_date:
                        current_start_date = final_start_date
                        
                    str_start = current_start_date.strftime("%Y-%m-%d")
                    str_end = current_end_date.strftime("%Y-%m-%d")
                    
                    # çµ±ä¸€ä½¿ç”¨ stable ç«¯é»ï¼Œå¸¶å…¥è¿´åœˆè¨ˆç®—å¥½çš„ from èˆ‡ to
                    url = f"{self.base_url}/stable/historical-price-eod/full?symbol={ticker}&from={str_start}&to={str_end}&apikey={self.api_key}"
                    
                    response = requests.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        chunk_data = response.json()
                        # FMP å›å‚³çš„è³‡æ–™å¯èƒ½æ˜¯ list æˆ– dictï¼Œå°‡å…¶ç„¡ç¸«æ¥åˆåˆ°å¤§å®¹å™¨ä¸­
                        if isinstance(chunk_data, list):
                            aggregated_data.extend(chunk_data)
                        elif isinstance(chunk_data, dict) and "historical" in chunk_data:
                            aggregated_data.extend(chunk_data["historical"])
                        elif isinstance(chunk_data, dict) and "data" in chunk_data:
                            aggregated_data.extend(chunk_data["data"])
                            
                    elif response.status_code == 429:
                        logger.warning("é”åˆ° API é€Ÿç‡é™åˆ¶ï¼Œæš«åœ 5 ç§’...")
                        time.sleep(5)
                        continue # è¿´åœˆä¸æ¨é€²ï¼Œé‡è©¦æ­¤å€é–“
                    else:
                        logger.error(f"æŠ“å– {ticker} ({str_start} è‡³ {str_end}) å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                        
                    # æ¨é€²æ™‚é–“è»¸ï¼Œæº–å‚™æŠ“å–ä¸Šä¸€å€‹ 5 å¹´ (å¾€å‰æ¨ 1 å¤©é¿å…æ—¥æœŸé‡ç–Š)
                    current_end_date = current_start_date - timedelta(days=1)
                
                # --- è¿´åœˆåˆ†å¡ŠæŠ“å–å®Œç•¢ï¼Œé–‹å§‹è™•ç†èˆ‡è½åœ° ---
                if aggregated_data:
                    # ğŸŒŸ æ­·å²å¤§çŒæ³¨ (>5000å¤©)ï¼Œå¯¦é«”å‚™ä»½ 30 å¹´çš„å®Œæ•´ JSON
                    if history_days > 5000:
                        backup_dir = Path("/workspace/raw_backup")
                        backup_dir.mkdir(exist_ok=True)
                        backup_file = backup_dir / f"{ticker}_30yr_backup.json.gz"
                        with gzip.open(backup_file, 'wt', encoding='utf-8') as f:
                            json.dump(aggregated_data, f)
                        logger.info(f"ğŸ’¾ å·²å°‡ {ticker} çš„ 30 å¹´åŸå§‹ JSON å£“ç¸®å‚™ä»½è‡³ç£ç¢Ÿ (å…± {len(aggregated_data)} ç­†)ã€‚")

                    # å°‡åˆä½µå¾Œçš„ List äº¤çµ¦åŸæœ‰çš„ parser è™•ç†
                    df = self._parse_fmp_json(aggregated_data, ticker)
                    
                    if df is not None and not df.empty:
                        db.upsert_market_data(df)
                        success_count += 1
                        
            except Exception as e:
                logger.error(f"æŠ“å– {ticker} æ™‚ç™¼ç”Ÿä¾‹å¤–éŒ¯èª¤: {str(e)}")
            
            finally:
                gc.collect()
                time.sleep(0.1) # æ‚¨çš„ Ultimate æ–¹æ¡ˆæœ‰ 3000 æ¬¡/åˆ†é¡åº¦ï¼Œ0.1 ç§’å»¶é²éå¸¸å®‰å…¨

        logger.info(f"è‚¡åƒ¹è³‡æ–™æŠ“å–å®Œç•¢ï¼æˆåŠŸ: {success_count}/{len(tickers)}")

    def _parse_fmp_json(self, data, ticker):
        """
        è§£æ FMP å›å‚³çš„ JSON ä¸¦æ¨™æº–åŒ–æ¬„ä½
        """
        df = None
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict) and "historical" in data:
            df = pd.DataFrame(data["historical"])
        elif isinstance(data, dict) and "data" in data:
            df = pd.DataFrame(data["data"])
            
        if df is not None and not df.empty:
            if 'symbol' not in df.columns:
                df['symbol'] = ticker
                
            # æ¬„ä½é‡æ–°å‘½åä»¥ç¬¦åˆ SQL Server Schema
            rename_map = {
                'date': 'Date', 'symbol': 'Symbol', 'open': 'Open',
                'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'
            }
            # åªä¿ç•™éœ€è¦çš„æ¬„ä½
            df_clean = df[list(set(rename_map.keys()).intersection(df.columns))].rename(columns=rename_map)
            df_clean['Date'] = pd.to_datetime(df_clean['Date'])
            return df_clean
        return None