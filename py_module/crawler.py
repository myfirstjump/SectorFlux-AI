import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from loguru import logger
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from py_module.config import Configuration
from py_module.database import DatabaseManipulation

class FinancialCrawler:

    def __init__(self, config):

        self.config = config
        self.api_key = config.FMP_API_KEY
        self.base_url = "https://financialmodelingprep.com"
        self.db = DatabaseManipulation(config)
        self.session = self._create_retry_session()

    def _create_retry_session(self):
        """
        ðŸ›¡ï¸ å»ºç«‹å…·å‚™æŒ‡æ•¸å›žé€€ (Exponential Backoff) çš„ Requests Session
        å°ˆé–€è™•ç† HTTP 429 (Too Many Requests) èˆ‡ç¶²è·¯æ³¢å‹•
        """

        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,  # ç­‰å¾…æ™‚é–“: 1s, 2s, 4s, 8s, 16s...
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )

        adapter = HTTPAdapter(max_retries=retries)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session



    def fetch_all_data(self, market='us', history_days=30):
        """ä¸»å…¥å£ï¼šåŸ·è¡Œå…¨å¸‚å ´çˆ¬å–ä»»å‹™ (å«æ‹†è‚¡åµæ¸¬èˆ‡è‡ªå‹•ä¿®å¾©)"""
        logger.info(f"ðŸš€ é–‹å§‹åŸ·è¡Œ {market.upper()} å¸‚å ´æ•¸æ“šä»»å‹™...")

        if market == 'us':
            macro_universe = getattr(self.config, 'MACRO_UNIVERSE', []) # åŠ å…¥å®è§€å› å­
            target_universe = self.config.get_all_tickers()
            target_universe = target_universe + macro_universe
            # target_universe = macro_universe  ### åªçˆ¬ MACRO çš„é …ç›®æ™‚æ‰“é–‹


            # === ðŸ›¡ï¸ éšŽæ®µä¸€ï¼šæ‹†è‚¡é›·é” (Split Radar) ===
            split_tickers = self._detect_splits(target_universe)

            # === ðŸš€ éšŽæ®µäºŒï¼šåŸ·è¡ŒæŠ“å– ===
            # å°‡æ‹†è‚¡æ¨™çš„ (éœ€é‡æŠ“ 30 å¹´) èˆ‡ æ­£å¸¸æ¨™çš„ (å¢žé‡) åˆ†æµ
            normal_tickers = [t for t in target_universe if t not in split_tickers]

            if split_tickers:
                logger.warning(f"ðŸš¨ ç™¼ç¾ {len(split_tickers)} æª”æ‹†è‚¡æ¨™çš„ï¼Œå•Ÿå‹• 30 å¹´ä¿®å¾©çŒæ³¨...")
                self._fetch_and_store_prices(split_tickers, history_days=10950) # 30å¹´

            if normal_tickers:
                logger.info(f"ðŸ“‹ é–‹å§‹åŸ·è¡Œ {len(normal_tickers)} æª”æ¨™çš„ä¹‹å¸¸è¦å¢žé‡æ›´æ–° ({history_days} å¤©)...")
                self._fetch_and_store_prices(normal_tickers, history_days)

    def _detect_splits(self, universe):

        """åµæ¸¬éŽåŽ» 7 å¤©æ˜¯å¦æœ‰æ‹†è‚¡äº‹ä»¶"""
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
            url = f"{self.base_url}/stable/splits-calendar?from={start_date}&to={end_date}&apikey={self.api_key}"
            resp = self.session.get(url, timeout=45)

            if resp.status_code == 200:
                splits = resp.json()
                split_symbols = [item.get('symbol') for item in splits if item.get('symbol')]

                # å›žå‚³äº¤é›† (Intersection)
                return list(set(split_symbols).intersection(set(universe)))
            return []

        except Exception as e:
            logger.error(f"âŒ æ‹†è‚¡åµæ¸¬å¤±æ•—: {e}")
            return []

    def _fetch_and_store_prices(self, tickers, history_days):

        """
        [æœ€çµ‚æ•´åˆç‰ˆ] é›™è»ŒæŠ“å– + 5å¹´åˆ†å¡Šæ©Ÿåˆ¶ + Stable API
        è§£æ±º 30 å¹´ä¸€æ¬¡è«‹æ±‚å°Žè‡´ FMP å›žå‚³ç©ºå€¼æˆ– Timeout çš„å•é¡Œ
        """

        import pandas as pd
        from datetime import datetime, timedelta
        import gc

        # FMP é™åˆ¶å»ºè­°ï¼šæ¯æ¬¡è«‹æ±‚ä¸è¶…éŽ 5 å¹´ (ç´„ 1825 å¤©)

        CHUNK_SIZE_DAYS = 1825

        for idx, symbol in enumerate(tickers):
            try:
                logger.info(f"[{idx+1}/{len(tickers)}] è™•ç† {symbol} (åˆ†å¡ŠæŠ“å– {history_days} å¤©)...")
                # æº–å‚™å¤§å®¹å™¨
                all_prices = []
                all_mcaps = []

                # è¨­å®šæ™‚é–“æ¸¸æ¨™
                end_date = datetime.now()
                start_date_limit = end_date - timedelta(days=history_days)

                cursor_end = end_date

                # === ðŸ”„ 5å¹´åˆ†å¡Šè¿´åœˆ (Chunk Loop) ===
                while cursor_end > start_date_limit:
                    cursor_start = cursor_end - timedelta(days=CHUNK_SIZE_DAYS)
                    if cursor_start < start_date_limit:
                        cursor_start = start_date_limit

                    # è½‰å­—ä¸²
                    str_start = cursor_start.strftime("%Y-%m-%d")
                    str_end = cursor_end.strftime("%Y-%m-%d")

                    # 1. æŠ“å–è‚¡åƒ¹ (åˆ†å¡Š)
                    price_url = f"{self.base_url}/stable/historical-price-eod/full"
                    payload = {
                        "symbol": symbol,
                        "from": str_start,
                        "to": str_end,
                        "apikey": self.api_key
                    }
                    try:
                        p_resp = self.session.get(price_url, params=payload, timeout=30).json()
                        if isinstance(p_resp, dict) and 'historical' in p_resp:
                            all_prices.extend(p_resp['historical'])
                        elif isinstance(p_resp, list): # æœ‰äº›ç«¯é»žç›´æŽ¥å›ž list
                            all_prices.extend(p_resp)
                    except Exception as e:
                        logger.warning(f"âš ï¸ {symbol} è‚¡åƒ¹åˆ†å¡Š {str_start}~{str_end} å¤±æ•—: {e}")

                    # 2. æŠ“å–å¸‚å€¼ (åˆ†å¡Š) - é›–ç„¶å¸‚å€¼ API åƒæ•¸å« limitï¼Œä½†æˆ‘å€‘å˜—è©¦å¸¶å…¥æ—¥æœŸå€é–“ä»¥æ±‚å°é½Š
                    # è‹¥ FMP å¸‚å€¼ API ä¸æ”¯æ´ from/toï¼Œå‰‡ fallback åˆ° limit æ¨¡å¼
                    # ä½†æ ¹æ“šç¶“é©—ï¼Œåˆ†å¡ŠæŠ“å–è¼ƒå®‰å…¨
                    mcap_url = f"{self.base_url}/stable/historical-market-capitalization?symbol={symbol}&from={str_start}&to={str_end}&apikey={self.api_key}"
                   
                    try:
                        m_resp = self.session.get(mcap_url, timeout=40).json()
                        if isinstance(m_resp, list):
                            all_mcaps.extend(m_resp)
                    except Exception as e:
                        pass # å¸‚å€¼å¤±æ•—ä¸ä¸­æ–·

                    # æ¸¸æ¨™å¾€å‰æŽ¨ (é¿å…é‡ç–Šï¼Œæ¸› 1 å¤©)
                    cursor_end = cursor_start - timedelta(days=1)
                    
                    # ç¦®è²Œæ€§å»¶é²
                    time.sleep(0.05)

                # === ðŸ§© æ•¸æ“šçµ„è£èˆ‡å¯«å…¥ ===
                if not all_prices:
                    logger.warning(f"âš ï¸ {symbol} å…¨éƒ¨åˆ†å¡Šçš†ç„¡è‚¡åƒ¹è³‡æ–™ï¼Œè·³éŽã€‚")
                    continue

                df_price = pd.DataFrame(all_prices)
                df_mcap = pd.DataFrame(all_mcaps) if all_mcaps else pd.DataFrame()

                # è³‡æ–™èžåˆ
                if not df_price.empty:
                    # ç¢ºä¿æ—¥æœŸæ ¼å¼
                    df_price['date'] = pd.to_datetime(df_price['date'])
                   
                    # åŽ»é‡ (åˆ†å¡Šé‚Šç•Œå¯èƒ½æœƒé‡è¤‡)
                    df_price.drop_duplicates(subset=['date'], inplace=True)
                    df_final = df_price.copy()

                    if not df_mcap.empty and 'date' in df_mcap.columns and 'marketCap' in df_mcap.columns:
                        df_mcap['date'] = pd.to_datetime(df_mcap['date'])
                        df_mcap.drop_duplicates(subset=['date'], inplace=True)
                        # Merge
                        df_final = pd.merge(df_price, df_mcap[['date', 'marketCap']], on='date', how='left')
                    else:
                        df_final['marketCap'] = None

                    # æ¬„ä½å°æ˜ 
                    df_final.rename(columns={
                        'date': 'Date', 'open': 'Open', 'high': 'High', 'low': 'Low',
                        'close': 'Close', 'volume': 'Volume', 'marketCap': 'Market_Cap'
                    }, inplace=True)
                
                    df_final['Symbol'] = symbol

                    # è¨ˆç®— Shares
                    def validate_and_fix(row):
                        price = row['Close']
                        mcap = row['Market_Cap']
                        if pd.notna(price) and pd.notna(mcap) and price != 0:
                            return mcap / price
                        return None

                    df_final['Shares_Outstanding'] = df_final.apply(validate_and_fix, axis=1)

                    # å¯«å…¥è³‡æ–™åº«
                    self.db.upsert_market_data(df_final)                   

                    # åžƒåœ¾å›žæ”¶
                    del df_price, df_mcap, df_final, all_prices, all_mcaps
                    gc.collect()

            except Exception as e:
                logger.error(f"âŒ è™•ç† {symbol} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")

    def fetch_etf_holdings(self, etf_list=None):

        """
        [ç¨ç«‹ä»»å‹™] æŠ“å– ETF æŒå€‰æ¬Šé‡
        å»ºè­°é »çŽ‡ï¼šæ¯é€±æˆ–æ¯æœˆåŸ·è¡Œä¸€æ¬¡
        """
        logger.info("ðŸ“¦ é–‹å§‹åŸ·è¡Œ ETF æŒå€‰æŠ“å–ä»»å‹™...")
      
        # å¦‚æžœæ²’æŒ‡å®šåå–®ï¼Œå°±æŠ“æ‰€æœ‰ç›£æŽ§ä¸­çš„æ¨™çš„ (API æœƒè‡ªå‹•éŽæ¿¾éž ETF)
        # ä½†ç‚ºäº†æ•ˆçŽ‡ï¼Œå»ºè­°æœ€å¥½å‚³å…¥æ˜Žç¢ºçš„ ETF æ¸…å–® (å¦‚ XLK, SPY...)
        if etf_list is None:
            etf_list = self.config.get_all_tickers()

        for symbol in etf_list:
            try:
                # ä½¿ç”¨ Stable ç«¯é»ž
                url = f"{self.base_url}/stable/etf-holdings?symbol={symbol}&apikey={self.api_key}"
                resp = self.session.get(url, timeout=10)               

                if resp.status_code == 200:
                    data = resp.json()
                    if not data: continue # ä¸æ˜¯ ETF æˆ–æ²’è³‡æ–™

                    # æ•´ç†è³‡æ–™
                    holdings_data = []
                    fetch_date = datetime.now().strftime("%Y-%m-%d")                   

                    for item in data:
                        holdings_data.append({
                            'Date': item.get('date', fetch_date), # è‹¥ API æ²’çµ¦æ—¥æœŸå°±ç”¨ç•¶å¤©
                            'ETF_Symbol': symbol,
                            'Holding_Symbol': item.get('asset'),
                            'Weight': item.get('weightPercentage'),
                            'Shares': item.get('sharesNumber')
                        })
                
                    if holdings_data:
                        df = pd.DataFrame(holdings_data)
                        logger.info(f"âœ… {symbol} æŠ“å–åˆ° {len(df)} æª”æŒå€‰")
                        # å‘¼å«å°ˆç”¨çš„ DB å¯«å…¥æ–¹æ³• (éœ€åœ¨ database.py æ–°å¢žå°æ‡‰æ–¹æ³•)
                        self.db.upsert_etf_holdings(df)
                       
                time.sleep(0.1) # ç¦®è²Œæ€§å»¶é²
              
            except Exception as e:
                logger.error(f"âŒ æŠ“å– ETF {symbol} æŒå€‰å¤±æ•—: {e}")