import pandas as pd
from datetime import datetime, timedelta
import requests
import os
from pathlib import Path
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

# --- 1. è¨­å®šèˆ‡é€£ç·š ---
load_dotenv()
API_KEY = os.environ.get("FMP_API_KEY")

# è³‡æ–™åº«é€£ç·šè¨­å®š (è«‹ç¢ºèªå¯†ç¢¼èˆ‡ docker-compose.yml ä¸€è‡´)
DB_USER = "sa"
DB_PASS = "SectorFlux_DB_2026!"
DB_HOST = "localhost"
DB_PORT = "1433"
DB_NAME = "SectorFluxDB"

# å»ºç«‹ SQLAlchemy Engine
connection_url = f"mssql+pyodbc://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes"
engine = create_engine(connection_url)

# å¿«å–ç›®éŒ„
CACHE_DIR = Path("./fmp_cache")
CACHE_DIR.mkdir(exist_ok=True)

# --- 2. çˆ¬èŸ²å‡½æ•¸ (FMP API) ---
def fetch_historical_prices(ticker, api_key, start_date="2023-01-01", end_date="2023-12-31"):
    """
    æŠ“å– FMP æ­·å²è‚¡åƒ¹ (Stable Endpoint)
    """
    # æª¢æŸ¥å¿«å– (ç‚ºäº†æ¸¬è©¦æ–¹ä¾¿ï¼Œæ‚¨å¯ä»¥éš¨æ™‚åˆªé™¤ cache è³‡æ–™å¤¾ä¾†å¼·åˆ¶é‡æŠ“)
    cache_file = CACHE_DIR / f"{ticker}_{start_date}_{end_date}.csv"
    if cache_file.exists():
        print(f"[Cache Hit] è®€å–æœ¬åœ°å¿«å–: {ticker}")
        return pd.read_csv(cache_file)

    url = f"https://financialmodelingprep.com/stable/historical-price-eod/full?symbol={ticker}&from={start_date}&to={end_date}&apikey={api_key}"
    print(f"[API Call] ä¸‹è¼‰æ•¸æ“šä¸­: {ticker}...")
    
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        
        # è™•ç† FMP å›å‚³æ ¼å¼
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict) and "historical" in data:
            df = pd.DataFrame(data["historical"])
        else:
            print(f"âŒ ç„¡æ³•è§£æ {ticker} çš„å›å‚³æ ¼å¼")
            return None

        if not df.empty:
            # ç¢ºä¿æœ‰ symbol æ¬„ä½ (æœ‰äº›ç«¯é»å›å‚³ä¸å¸¶ symbol)
            if 'symbol' not in df.columns:
                df['symbol'] = ticker
                
            df.to_csv(cache_file, index=False)
            return df
            
    except Exception as e:
        print(f"âŒ API è«‹æ±‚å¤±æ•—: {e}")
    return None

# --- 3. è³‡æ–™æ¸…æ´—èˆ‡è½‰æ› (Transform) ---
def transform_data(df):
    """
    å°‡ DataFrame æ¬„ä½åç¨±è½‰æ›ç‚ºç¬¦åˆ SQL Server Schema çš„æ ¼å¼
    """
    # é¸æ“‡éœ€è¦çš„æ¬„ä½ä¸¦é‡æ–°å‘½å
    # FMP å›å‚³é€šå¸¸æ˜¯ lowercase, æˆ‘å€‘ DB è¨­è¨ˆæ˜¯ PascalCase
    rename_map = {
        'date': 'Date',
        'symbol': 'Symbol',
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume'
    }
    
    # éæ¿¾ä¸¦é‡æ–°å‘½å
    df_clean = df[rename_map.keys()].rename(columns=rename_map).copy()
    
    # ç¢ºä¿ Date æ˜¯æ—¥æœŸæ ¼å¼
    df_clean['Date'] = pd.to_datetime(df_clean['Date'])
    
    return df_clean

# --- 4. æ ¸å¿ƒ Upsert é‚è¼¯ (Load) ---
def upsert_to_sql(df, table_name="Fact_DailyPrice"):
    """
    ä½¿ç”¨ Staging Table + MERGE èªæ³•é€²è¡Œé«˜æ•ˆ Upsert
    """
    if df is None or df.empty:
        return

    # ç”¢ç”Ÿä¸€å€‹éš¨æ©Ÿæˆ–å›ºå®šçš„æš«å­˜è¡¨åç¨±
    staging_table = f"Staging_{table_name}"
    
    with engine.connect() as conn:
        trans = conn.begin() # é–‹å•Ÿäº¤æ˜“
        try:
            # A. å°‡è³‡æ–™å¯«å…¥æš«å­˜è¡¨ (å¦‚æœå­˜åœ¨å‰‡å–ä»£)
            print(f"â³ æ­£åœ¨å¯«å…¥æš«å­˜è¡¨ {staging_table} ({len(df)} ç­†)...")
            df.to_sql(staging_table, con=conn, if_exists='replace', index=False)
            
            # B. åŸ·è¡Œ MERGE SQL æŒ‡ä»¤
            # é€™æ®µ SQL æ˜¯ Upsert çš„éˆé­‚ï¼š
            # ç•¶ Date èˆ‡ Symbol ç›¸åŒæ™‚ -> æ›´æ–°åƒ¹æ ¼ (è¦†è“‹èˆŠè³‡æ–™ï¼Œè§£æ±ºæ‹†è‚¡ä¿®æ­£)
            # ç•¶æ‰¾ä¸åˆ°æ™‚ -> æ’å…¥æ–°è³‡æ–™
            merge_sql = text(f"""
            MERGE INTO {table_name} AS target
            USING {staging_table} AS source
            ON target.Date = source.Date AND target.Symbol = source.Symbol
            
            WHEN MATCHED THEN
                UPDATE SET 
                    target.[Open] = source.[Open],
                    target.High = source.High,
                    target.Low = source.Low,
                    target.[Close] = source.[Close],
                    target.Volume = source.Volume
            
            WHEN NOT MATCHED THEN
                INSERT (Date, Symbol, [Open], High, Low, [Close], Volume)
                VALUES (source.Date, source.Symbol, source.[Open], source.High, source.Low, source.[Close], source.Volume);
            """)
            
            conn.execute(merge_sql)
            
            # C. åˆªé™¤æš«å­˜è¡¨
            conn.execute(text(f"DROP TABLE {staging_table}"))
            
            trans.commit()
            print(f"âœ… Upsert æˆåŠŸï¼å·²åŒæ­¥ {len(df)} ç­†è³‡æ–™è‡³ {table_name}ã€‚")
            
        except Exception as e:
            trans.rollback()
            print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
            raise e

# --- 5. ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    tickers = ["AAPL", "MSFT"]
    
    # å‹•æ…‹è¨ˆç®—æ—¥æœŸè¦–çª—
    today = datetime.now()
    end_date_str = today.strftime("%Y-%m-%d")
    
    # å¾€å‰æ¨ 30 å¤©ä½œç‚º start_date (è‡ªå‹•ä¿®æ­£è¿‘æœŸå¯èƒ½çš„æ‹†è‚¡èˆ‡é™¤æ¯)
    start_date_str = (today - timedelta(days=30)).strftime("%Y-%m-%d")
    
    for ticker in tickers:
        print(f"\nğŸš€ é–‹å§‹è™•ç†: {ticker} (æ“·å–å€é–“: {start_date_str} è‡³ {end_date_str})")
        
        # 1. Extract (å‚³å…¥å‹•æ…‹æ—¥æœŸ)
        raw_df = fetch_historical_prices(ticker, API_KEY, start_date=start_date_str, end_date=end_date_str)
        
        if raw_df is not None:
            # 2. Transform
            clean_df = transform_data(raw_df)
            
            # 3. Load (Upsert)
            upsert_to_sql(clean_df)
            
    print("\nğŸ‰ æ‰€æœ‰ä½œæ¥­å®Œæˆï¼æ‚¨çš„ SectorFlux è³‡æ–™åº«å·²æ›´æ–°å®Œç•¢ã€‚")